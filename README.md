# ðŸ§  Improving Evolutionary Neural Architecture Search for Image Classification Accuracy
---

## ðŸ“˜ Overview

This project implements and extends **Evolutionary Neural Architecture Search (ENAS)** for **image classification**, focusing on improving accuracy and computational efficiency using **CIFAR-10** and **CIFAR-100** datasets.  

Traditional NAS methods are resource-intensive and slow. This work integrates **pretrained backbones**, **progressive evolution**, **Hyperband tuning**, and **search-space pruning** to design a faster and more accurate ENAS pipeline.

---

## ðŸš€ Key Contributions

- **Enhanced ENAS Framework:** Combines **aging evolution**, **weight sharing**, and **progressive search** to improve exploration and convergence.  
- **Pre-trained Knowledge Transfer:** Incorporates **MobileNetV2** and **custom deep CNNs** as frozen backbones to accelerate architecture discovery.  
- **Search-Space Reduction:** Filters low-performing architectures to focus computation on high-potential designs.  
- **Hyperparameter Optimization:** Utilizes **Keras Tuner (Hyperband / RandomSearch)** for efficient NAS exploration.  
- **Scalable Evaluation:** Provides modular testing across multiple CIFAR batches with performance visualization.

---

## ðŸ§© Project Structure

```bash
â”œâ”€â”€ NASTest.py
â”‚   â”œâ”€ RandomSearch NAS on CIFAR-10 (baseline NAS)
â”‚   â””â”€ Visualizes predicted vs actual classes.
â”‚
â”œâ”€â”€ ENASTest.py
â”‚   â”œâ”€ NAS with pretrained MobileNetV2 backbone.
â”‚   â””â”€ Uses Hyperband for accelerated tuning.
â”‚
â”œâ”€â”€ ENAS_Hyperband_DeeperPretrain_Full.py
â”‚   â”œâ”€ Loads full CIFAR-10 dataset.
â”‚   â”œâ”€ Pre-trains deeper CNN for feature transfer.
â”‚   â”œâ”€ Runs ENAS via Hyperband using pretrained backbone.
â”‚   â”œâ”€ Retrains best model and evaluates on test data.
â”‚   â””â”€ Saves models and logs to `/nas_output/`.
